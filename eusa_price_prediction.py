# -*- coding: utf-8 -*-
"""EUSA_PRICE_PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xh7W5QEYkNbnyfYlNLqylibZjBK-WcWC
"""

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

print(tf.__version__)


# Make numpy printouts easier to read.
np.set_printoptions(precision=3, suppress=True)

column_names = ["Category","Rating","Distance","Cost"]

raw_dataset = pd.read_csv('drive/My Drive/Data.csv', names=column_names,
                          na_values='?', comment='\t',skiprows=1,
                          sep=',', skipinitialspace=True)

dataset = raw_dataset.copy()

dataset.isna().sum()
dataset = dataset.dropna()

dataset = dataset.iloc[1:-1]


#print(dataset)

dataset['Category'] = dataset['Category'].map({'Carpenter': 'Carpenter' , 'Plumber': 'Plumber', 'Mechanic': 'Mechanic', 'Electrician': 'Electrician'})

dataset = pd.get_dummies(dataset, columns=['Category'], prefix='', prefix_sep='')

dataset['Rating'] = pd.to_numeric(dataset['Rating'], errors='coerce')
dataset['Distance'] = pd.to_numeric(dataset['Distance'], errors='coerce')

#print(dataset['Rating'])
#print(dataset['Distance'])


train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)


#print(train_dataset)
#print(test_dataset)

#sns.pairplot(train_dataset[["Cost","Rating", "Distance"]], diag_kind='kde')
#print(train_dataset.describe().transpose())

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('Cost')
test_labels = test_features.pop('Cost')

#print(train_dataset.describe().transpose()[['mean', 'std']])

normalizer = preprocessing.Normalization()
normalizer.adapt(np.array(train_features,dtype=float))
#print(normalizer.mean.numpy())

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error')
  plt.legend()
  plt.grid(True)

linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

print(linear_model.predict(train_features[:10]))
print(linear_model.layers[1].kernel)

linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.05),
    loss='mean_absolute_error')

#history = linear_model.fit(
#    train_features, train_labels, 
#    epochs=1000,
    # suppress logging
#    verbose=0,
    # Calculate validation results on 20% of the training data
#    validation_split = 0.2)
#plot_loss(history)



def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

history = dnn_model.fit(
    train_features, train_labels,
    validation_split=0.2,
    verbose=0, epochs=1000)

plot_loss(history)

test_results = dnn_model.evaluate(test_features, test_labels, verbose=0)
print(dnn_model.predict(train_features[:10]))
print(train_features[:10])

test_predictions = dnn_model.predict(test_features).flatten()
print(test_labels,test_predictions)
a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 1500]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

dnn_model.save('drive/MyDrive/dnn_model')

converter = tf.lite.TFLiteConverter.from_keras_model(dnn_model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)

print(dnn_model.predict(train_features[:10]))
print(train_labels[:10])